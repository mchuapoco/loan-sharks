{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "northern-halloween",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "# import holoviews as hv\n",
    "# from holoviews import dim, opts\n",
    "# import bokeh.io\n",
    "# import iqplot\n",
    "import urllib\n",
    "import re\n",
    "import helper\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "interested-protein",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'helper' from '/Users/prakash/Desktop/Courses/ML/loan-sharks/poems/helper.py'>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this to reload helper.py so you don't have to restart the kernel\n",
    "import importlib\n",
    "\n",
    "importlib.reload(helper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extraordinary-enclosure",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "surface-montgomery",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words, all_sequences, word_dict, all_sonnet_int, syllable_dict= helper.getAllWordsAndSequences(\"data/shakespeare.txt\", \"data/Syllable_dictionary.txt\",syllable_count=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "boolean-layer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1522, 2210, 457, 2683, 1506, 2114, 609, 3205]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sonnet_int[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "terminal-killer",
   "metadata": {},
   "source": [
    "## Utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "czech-packet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_map_reverser(obs_map):\n",
    "    obs_map_r = {}\n",
    "\n",
    "    for key in obs_map:\n",
    "        obs_map_r[obs_map[key]] = key\n",
    "\n",
    "    return obs_map_r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-dryer",
   "metadata": {},
   "source": [
    "## Training HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "varying-valuation",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(all_sonnet_int)):\n",
    "    if len(all_sonnet_int[idx])>1:\n",
    "        new_sonet_sequence.append(all_sonnet_int[idx][-2::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "detailed-stations",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "Iteration: 60\n",
      "Iteration: 70\n",
      "Iteration: 80\n",
      "Iteration: 90\n",
      "Iteration: 100\n"
     ]
    }
   ],
   "source": [
    "hmm5 = helper.unsupervised_HMM(new_sonet_sequence, 30, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-bicycle",
   "metadata": {},
   "source": [
    "## Rhyme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "destroyed-threshold",
   "metadata": {},
   "outputs": [],
   "source": [
    "rhyme=[]\n",
    "rhyme_couplet=[]\n",
    "new_sonet_sequence=[]\n",
    "for idx in range(len(all_sonnet_int)):\n",
    "    if (idx+2)%14==0 and len(all_sonnet_int[idx+1])>1:\n",
    "        rhyme_couplet.append((all_sonnet_int[idx][-2],all_sonnet_int[idx+1][-2]))\n",
    "    elif idx%4==0 and len(all_sonnet_int[idx+2])>1:\n",
    "        rhyme.append((all_sonnet_int[idx][-2],all_sonnet_int[idx+2][-2]))\n",
    "        rhyme.append((all_sonnet_int[idx+1][-2],all_sonnet_int[idx+3][-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "constitutional-plymouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_rev_sonents(hmm, obs_map, syllable_dict, n_sonnet=10, start_word=None):\n",
    "    # Get reverse map.\n",
    "    obs_map_r = obs_map_reverser(obs_map)\n",
    "    start_state = None\n",
    "    if start_word:\n",
    "        emition_pos=[hmm.O[i][start_word] for i in range(hmm.L)]\n",
    "        start_state=random.choices([i for i in range(hmm.L)],weights=emition_pos)[0]\n",
    "    # Sample and convert sentence.\n",
    "    emission = []\n",
    "    states = []\n",
    "    count_sonet = int(syllable_dict[obs_map_r[start_word]][-1])\n",
    "    state=start_state\n",
    "#     print(obs_map_r[start_word])\n",
    "\n",
    "    for i in range(100):\n",
    "            # Append state.\n",
    "            states.append(state)\n",
    "            while(True):\n",
    "\n",
    "                # Sample next observation.\n",
    "                rand_var = random.uniform(0, 1)\n",
    "                next_obs = 0\n",
    "\n",
    "                while rand_var > 0:\n",
    "                    rand_var -= hmm.O[state][next_obs]\n",
    "                    next_obs += 1\n",
    "\n",
    "                next_obs -= 1\n",
    "                emission.append(next_obs)\n",
    "                if i!=0:\n",
    "                    count_sonet+=int(syllable_dict[obs_map_r[next_obs]][-1])\n",
    "                if count_sonet==10:\n",
    "                    break\n",
    "                elif count_sonet>10:\n",
    "                    count_sonet-=int(syllable_dict[obs_map_r[next_obs]][-1])\n",
    "                    emission.pop()\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "            if count_sonet==10:\n",
    "                break\n",
    "\n",
    "            # Sample next state.\n",
    "            rand_var = random.uniform(0, 1)\n",
    "            next_state = 0\n",
    "\n",
    "            while rand_var > 0:\n",
    "                rand_var -= hmm.A[state][next_state]\n",
    "                next_state += 1\n",
    "\n",
    "            next_state -= 1\n",
    "            state = next_state\n",
    "    sentence = [obs_map_r[i] for i in emission]\n",
    "    \n",
    "    if start_word:\n",
    "        sentence[0]=obs_map_r[start_word]\n",
    "\n",
    "    return ' '.join(sentence[::-1]).capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "hawaiian-anderson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Sonnet:\n",
      "====================\n",
      "Earth when in show the a powerful confound\n",
      "Conscience in this and pays a learned's pain\n",
      "Best still darkening the joy unless i'll brow\n",
      "Ten yet in music looks shall with my east\n",
      "Bending with her owner's due that your view\n",
      "Were would to sweet'st gone that your winter's trust\n",
      "Do name thief thy image with o in new\n",
      "Mayst in power lilies heavy with of me\n",
      "To be distilled and all that is that face\n",
      "Away of brain lawful end string ride missed\n",
      "Little fair muse for so am breed so place\n",
      "Pain and will foul be what expense of score\n",
      "Truth to 'gainst doth face the of of worth gone\n",
      "Theirs which me sleep in new thou be alone\n"
     ]
    }
   ],
   "source": [
    "print('Sample Sonnet:\\n====================')\n",
    "sentence_rhyme=['' for i in range(14)]\n",
    "mapper=obs_map_reverser(word_dict)\n",
    "for i in range(3):\n",
    "    new_rhyme_1=random.choice(rhyme)\n",
    "    new_rhyme_2=random.choice(rhyme)\n",
    "    sentence_rhyme[i*4]=new_rhyme_1[0]\n",
    "    sentence_rhyme[i*4+1]=new_rhyme_2[0]\n",
    "    sentence_rhyme[i*4+2]=new_rhyme_1[1]\n",
    "    sentence_rhyme[i*4+3]=new_rhyme_2[1]\n",
    "#     print(new_rhyme_1,new_rhyme_2)\n",
    "new_rhyme_1=random.choice(rhyme_couplet)\n",
    "sentence_rhyme[-2]=new_rhyme_1[0]\n",
    "sentence_rhyme[-1]=new_rhyme_1[1]\n",
    "# print(mapper[new_rhyme_1[0]],mapper[new_rhyme_1[1]])\n",
    "for start_rhyme in sentence_rhyme:\n",
    "    print(sample_rev_sonents(hmm5, word_dict, syllable_dict, n_sonnet=10, start_word=start_rhyme))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessory-portrait",
   "metadata": {},
   "source": [
    "## Meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dressed-singapore",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to /Users/prakash/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/cmudict.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('cmudict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ordered-divide",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import cmudict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "respiratory-acrylic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stress_syllable(word,prondict):\n",
    "    start,end=-1,-1\n",
    "    if word not in prondict:\n",
    "        return start,end\n",
    "    processed=[ele[-1] for ele in prondict[word][0]]\n",
    "    for ele in processed:\n",
    "        if ele.isnumeric():\n",
    "            start=int(ele)\n",
    "            break\n",
    "    for ele in processed[::-1]:\n",
    "        if ele.isnumeric():\n",
    "            end=int(ele)\n",
    "            break\n",
    "    return start,end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "animal-physiology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prondict = cmudict.dict()\n",
    "stress_syllable('compare',prondict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "adjacent-burke",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_rev_sonents_meter(hmm, obs_map, syllable_dict, prondict, n_sonnet=10, start_word=None):\n",
    "    # Get reverse map.\n",
    "    obs_map_r = obs_map_reverser(obs_map)\n",
    "    start_state = None\n",
    "    if start_word:\n",
    "        emition_pos=[hmm.O[i][start_word] for i in range(hmm.L)]\n",
    "        start_state=random.choices([i for i in range(hmm.L)],weights=emition_pos)[0]\n",
    "    # Sample and convert sentence.\n",
    "    emission = []\n",
    "    states = []\n",
    "    count_sonet = int(syllable_dict[obs_map_r[start_word]][-1])\n",
    "    start,_ = stress_syllable(obs_map_r[start_word],prondict)\n",
    "    state=start_state\n",
    "#     print(obs_map_r[start_word])\n",
    "\n",
    "    for i in range(100):\n",
    "            # Append state.\n",
    "            states.append(state)\n",
    "            while(True):\n",
    "\n",
    "                # Sample next observation.\n",
    "                rand_var = random.uniform(0, 1)\n",
    "                next_obs = 0\n",
    "\n",
    "                while rand_var > 0:\n",
    "                    rand_var -= hmm.O[state][next_obs]\n",
    "                    next_obs += 1\n",
    "\n",
    "                next_obs -= 1\n",
    "                start_new,end_new = stress_syllable(obs_map_r[next_obs],prondict)\n",
    "                if end_new!=start:\n",
    "                    continue\n",
    "                start=start_new\n",
    "                emission.append(next_obs)\n",
    "                if i!=0:\n",
    "                    count_sonet+=int(syllable_dict[obs_map_r[next_obs]][-1])\n",
    "                if count_sonet==10:\n",
    "                    break\n",
    "                elif count_sonet>10:\n",
    "                    count_sonet-=int(syllable_dict[obs_map_r[next_obs]][-1])\n",
    "                    emission.pop()\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "            if count_sonet==10:\n",
    "                break\n",
    "\n",
    "            # Sample next state.\n",
    "            rand_var = random.uniform(0, 1)\n",
    "            next_state = 0\n",
    "\n",
    "            while rand_var > 0:\n",
    "                rand_var -= hmm.A[state][next_state]\n",
    "                next_state += 1\n",
    "\n",
    "            next_state -= 1\n",
    "            state = next_state\n",
    "    sentence = [obs_map_r[i] for i in emission]\n",
    "    \n",
    "    if start_word:\n",
    "        sentence[0]=obs_map_r[start_word]\n",
    "\n",
    "    return ' '.join(sentence[::-1]).capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "imposed-albany",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Sonnet:\n",
      "====================\n",
      "Sable towards eyes which thine knows of their name\n",
      "Ills him their with my faults it old or true\n",
      "Do my sword with my with weep to all age\n",
      "Name was my this young that it indigest\n",
      "Left o grew not that i men's to endure\n",
      "My deeds or one that long be for of glass\n",
      "Me should to times me when be such woe cure\n",
      "Thy mind thy thanks of thy dear my love was\n",
      "Love than heart full this nothing within grace\n",
      "Hate did by thee his mind's that my love is\n",
      "Grave lips this but love on my large my black\n",
      "Of it are at from how be rest amis\n",
      "Bosom disgrace of thy sweet i from one\n",
      "Nor o love will i have seen fair alone\n"
     ]
    }
   ],
   "source": [
    "print('Sample Sonnet:\\n====================')\n",
    "sentence_rhyme=['' for i in range(14)]\n",
    "mapper=obs_map_reverser(word_dict)\n",
    "for i in range(3):\n",
    "    new_rhyme_1=random.choice(rhyme)\n",
    "    new_rhyme_2=random.choice(rhyme)\n",
    "    sentence_rhyme[i*4]=new_rhyme_1[0]\n",
    "    sentence_rhyme[i*4+1]=new_rhyme_2[0]\n",
    "    sentence_rhyme[i*4+2]=new_rhyme_1[1]\n",
    "    sentence_rhyme[i*4+3]=new_rhyme_2[1]\n",
    "#     print(new_rhyme_1,new_rhyme_2)\n",
    "new_rhyme_1=random.choice(rhyme_couplet)\n",
    "sentence_rhyme[-2]=new_rhyme_1[0]\n",
    "sentence_rhyme[-1]=new_rhyme_1[1]\n",
    "# print(mapper[new_rhyme_1[0]],mapper[new_rhyme_1[1]])\n",
    "for start_rhyme in sentence_rhyme:\n",
    "    print(sample_rev_sonents_meter(hmm5, word_dict, syllable_dict, prondict, n_sonnet=10, start_word=start_rhyme))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-union",
   "metadata": {},
   "source": [
    "## Mixing Spenser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "national-change",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
