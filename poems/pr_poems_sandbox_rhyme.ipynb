{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "rough-fleece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import urllib\n",
    "import re\n",
    "import helper\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "planned-producer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'helper' from '/Users/prakash/Desktop/Courses/ML/loan-sharks/poems/helper.py'>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this to reload helper.py so you don't have to restart the kernel\n",
    "import importlib\n",
    "\n",
    "importlib.reload(helper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-hepatitis",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "mighty-defendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words, all_sequences, word_dict, all_sonnet_int, syllable_dict= helper.getAllWordsAndSequences(\"data/shakespeare.txt\", \"data/Syllable_dictionary.txt\",syllable_count=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "swedish-malawi",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[905, 2956, 96, 480, 2870, 2070, 2474, 3205]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_sonnet_int[11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-label",
   "metadata": {},
   "source": [
    "## Utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "tender-egypt",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obs_map_reverser(obs_map):\n",
    "    obs_map_r = {}\n",
    "\n",
    "    for key in obs_map:\n",
    "        obs_map_r[obs_map[key]] = key\n",
    "\n",
    "    return obs_map_r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intensive-blanket",
   "metadata": {},
   "source": [
    "## Naive HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "devoted-given",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sonnet_int_new=[]\n",
    "for i in range(0,len(all_sonnet_int),14):\n",
    "    temp=[]\n",
    "    for j in range(14):\n",
    "        temp.extend(all_sonnet_int[i+j])\n",
    "    all_sonnet_int_new.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "running-module",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "Iteration: 60\n",
      "Iteration: 70\n",
      "Iteration: 80\n",
      "Iteration: 90\n",
      "Iteration: 100\n"
     ]
    }
   ],
   "source": [
    "hmm_naive = helper.unsupervised_HMM(all_sonnet_int_new, 10, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "shaped-program",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Sentence:\n",
      "====================\n",
      "Full friend \n",
      " make aught perfect that and now ruining \n",
      " bear still \n",
      " that flattery and costly gav'st moan on which what dignified \n",
      " say thou of i forbear makes hast doom of blushing \n",
      " call proof \n",
      " to heaven honouring her the thee jacks but all of truth \n",
      " her accents \n",
      " or praises whose mine that from no they \n",
      " cannot three my my \n",
      " thee a pleasure \n",
      " but so \n",
      " thee thou of self-substantial jealousy may like less \n",
      " thou him hours to which but verse flattered and o'er-green my universe be night \n",
      " but like love be sober should what blessed of set familiar her what as are from i \n",
      " fawn \n",
      " for \n",
      " as adjunct \n",
      " why hawks appear \n",
      " be wide love husbandry \n",
      " in miracle tongue-tied me line proud \n",
      " be \n",
      " to else is is the with what i plague from mine a \n",
      " as kind new thy fortune's sight \n",
      " peep eyes although my \n",
      " the i it be the seals want tempests thou wilt they in or a alone \n",
      " the days it morrow the in some wretched cannot pity how his smell \n",
      " when decrepit they...\n"
     ]
    }
   ],
   "source": [
    "print('Sample Sentence:\\n====================')\n",
    "print(helper.sample_sentence(hmm_naive, word_dict, n_words=200))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-affect",
   "metadata": {},
   "source": [
    "## Training HMM in reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "traditional-liberty",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sonet_sequence=[]\n",
    "for idx in range(len(all_sonnet_int)):\n",
    "    if len(all_sonnet_int[idx])>1:\n",
    "        new_sonet_sequence.append(all_sonnet_int[idx][-2::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "supreme-traveler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "Iteration: 60\n",
      "Iteration: 70\n",
      "Iteration: 80\n",
      "Iteration: 90\n",
      "Iteration: 100\n"
     ]
    }
   ],
   "source": [
    "hmm_reverse = helper.unsupervised_HMM(new_sonet_sequence, 30, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-temple",
   "metadata": {},
   "source": [
    "## Rhyme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "resistant-latvia",
   "metadata": {},
   "outputs": [],
   "source": [
    "rhyme=[]\n",
    "rhyme_couplet=[]\n",
    "new_sonet_sequence=[]\n",
    "for idx in range(len(all_sonnet_int)):\n",
    "    if (idx+2)%14==0 and len(all_sonnet_int[idx+1])>1:\n",
    "        rhyme_couplet.append((all_sonnet_int[idx][-2],all_sonnet_int[idx+1][-2]))\n",
    "    elif idx%4==0 and len(all_sonnet_int[idx+2])>1:\n",
    "        rhyme.append((all_sonnet_int[idx][-2],all_sonnet_int[idx+2][-2]))\n",
    "        rhyme.append((all_sonnet_int[idx+1][-2],all_sonnet_int[idx+3][-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "periodic-brooklyn",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_rev_sonents(hmm, obs_map, syllable_dict, n_sonnet=10, start_word=None):\n",
    "    # Get reverse map.\n",
    "    obs_map_r = obs_map_reverser(obs_map)\n",
    "    start_state = None\n",
    "    if start_word:\n",
    "        emition_pos=[hmm.O[i][start_word] for i in range(hmm.L)]\n",
    "        start_state=random.choices([i for i in range(hmm.L)],weights=emition_pos)[0]\n",
    "    # Sample and convert sentence.\n",
    "    emission = []\n",
    "    states = []\n",
    "    count_sonet = int(syllable_dict[obs_map_r[start_word]][-1])\n",
    "    state=start_state\n",
    "#     print(obs_map_r[start_word])\n",
    "\n",
    "    for i in range(100):\n",
    "            # Append state.\n",
    "            states.append(state)\n",
    "            while(True):\n",
    "\n",
    "                # Sample next observation.\n",
    "                rand_var = random.uniform(0, 1)\n",
    "                next_obs = 0\n",
    "\n",
    "                while rand_var > 0:\n",
    "                    rand_var -= hmm.O[state][next_obs]\n",
    "                    next_obs += 1\n",
    "\n",
    "                next_obs -= 1\n",
    "                emission.append(next_obs)\n",
    "                if i!=0:\n",
    "                    count_sonet+=int(syllable_dict[obs_map_r[next_obs]][-1])\n",
    "                if count_sonet==10:\n",
    "                    break\n",
    "                elif count_sonet>10:\n",
    "                    count_sonet-=int(syllable_dict[obs_map_r[next_obs]][-1])\n",
    "                    emission.pop()\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "            if count_sonet==10:\n",
    "                break\n",
    "\n",
    "            # Sample next state.\n",
    "            rand_var = random.uniform(0, 1)\n",
    "            next_state = 0\n",
    "\n",
    "            while rand_var > 0:\n",
    "                rand_var -= hmm.A[state][next_state]\n",
    "                next_state += 1\n",
    "\n",
    "            next_state -= 1\n",
    "            state = next_state\n",
    "    sentence = [obs_map_r[i] for i in emission]\n",
    "    \n",
    "    if start_word:\n",
    "        sentence[0]=obs_map_r[start_word]\n",
    "\n",
    "    return ' '.join(sentence[::-1]).capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "violent-spider",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Sonnet:\n",
      "====================\n",
      "Ne'er love leave be hand in another change\n",
      "Soul my roses for you taste fair course gone\n",
      "Self to fly o me age his damasked strange\n",
      "Self nor for memorial numbers alone\n",
      "Eyes like other discased than would disarmed\n",
      "Was i than you she truth posterity\n",
      "Thy monument when thy perpetual\n",
      "As to thou doth dry thou ride so the heir\n",
      "I of thy impression of because knife\n",
      "Arising other verse so to whence ill\n",
      "On made beseechers list goodness when life\n",
      "Pleasure shall lusty love confined again\n",
      "Deem over-partial two and blame your life\n",
      "To but me scope nor perjured and thy knife\n"
     ]
    }
   ],
   "source": [
    "print('Sample Sonnet:\\n====================')\n",
    "sentence_rhyme=['' for i in range(14)]\n",
    "mapper=obs_map_reverser(word_dict)\n",
    "for i in range(3):\n",
    "    new_rhyme_1=random.choice(rhyme)\n",
    "    new_rhyme_2=random.choice(rhyme)\n",
    "    sentence_rhyme[i*4]=new_rhyme_1[0]\n",
    "    sentence_rhyme[i*4+1]=new_rhyme_2[0]\n",
    "    sentence_rhyme[i*4+2]=new_rhyme_1[1]\n",
    "    sentence_rhyme[i*4+3]=new_rhyme_2[1]\n",
    "#     print(new_rhyme_1,new_rhyme_2)\n",
    "new_rhyme_1=random.choice(rhyme_couplet)\n",
    "sentence_rhyme[-2]=new_rhyme_1[0]\n",
    "sentence_rhyme[-1]=new_rhyme_1[1]\n",
    "# print(mapper[new_rhyme_1[0]],mapper[new_rhyme_1[1]])\n",
    "for start_rhyme in sentence_rhyme:\n",
    "    print(sample_rev_sonents(hmm_reverse, word_dict, syllable_dict, n_sonnet=10, start_word=start_rhyme))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-chest",
   "metadata": {},
   "source": [
    "## Meter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "competitive-salem",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package cmudict to /Users/prakash/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('cmudict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "synthetic-concert",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import cmudict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fixed-shadow",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stress_syllable(word,prondict):\n",
    "    start,end=-1,-1\n",
    "    if word not in prondict:\n",
    "        return start,end\n",
    "    processed=[ele[-1] for ele in prondict[word][0]]\n",
    "    for ele in processed:\n",
    "        if ele.isnumeric():\n",
    "            start=int(ele)\n",
    "            break\n",
    "    for ele in processed[::-1]:\n",
    "        if ele.isnumeric():\n",
    "            end=int(ele)\n",
    "            break\n",
    "    return start,end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "pharmaceutical-skirt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prondict = cmudict.dict()\n",
    "stress_syllable('compare',prondict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "instrumental-airplane",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_rev_sonents_meter(hmm, obs_map, syllable_dict, prondict, n_sonnet=10, start_word=None):\n",
    "    # Get reverse map.\n",
    "    obs_map_r = obs_map_reverser(obs_map)\n",
    "    start_state = None\n",
    "    if start_word:\n",
    "        emition_pos=[hmm.O[i][start_word] for i in range(hmm.L)]\n",
    "        start_state=random.choices([i for i in range(hmm.L)],weights=emition_pos)[0]\n",
    "    # Sample and convert sentence.\n",
    "    emission = []\n",
    "    states = []\n",
    "    count_sonet = int(syllable_dict[obs_map_r[start_word]][-1])\n",
    "    start,_ = stress_syllable(obs_map_r[start_word],prondict)\n",
    "    state=start_state\n",
    "#     print(obs_map_r[start_word])\n",
    "\n",
    "    for i in range(100):\n",
    "            # Append state.\n",
    "            states.append(state)\n",
    "            while(True):\n",
    "\n",
    "                # Sample next observation.\n",
    "                rand_var = random.uniform(0, 1)\n",
    "                next_obs = 0\n",
    "\n",
    "                while rand_var > 0:\n",
    "                    rand_var -= hmm.O[state][next_obs]\n",
    "                    next_obs += 1\n",
    "\n",
    "                next_obs -= 1\n",
    "                start_new,end_new = stress_syllable(obs_map_r[next_obs],prondict)\n",
    "                if end_new!=start:\n",
    "                    continue\n",
    "                start=start_new\n",
    "                emission.append(next_obs)\n",
    "                if i!=0:\n",
    "                    count_sonet+=int(syllable_dict[obs_map_r[next_obs]][-1])\n",
    "                if count_sonet==10:\n",
    "                    break\n",
    "                elif count_sonet>10:\n",
    "                    count_sonet-=int(syllable_dict[obs_map_r[next_obs]][-1])\n",
    "                    emission.pop()\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "            if count_sonet==10:\n",
    "                break\n",
    "\n",
    "            # Sample next state.\n",
    "            rand_var = random.uniform(0, 1)\n",
    "            next_state = 0\n",
    "\n",
    "            while rand_var > 0:\n",
    "                rand_var -= hmm.A[state][next_state]\n",
    "                next_state += 1\n",
    "\n",
    "            next_state -= 1\n",
    "            state = next_state\n",
    "    sentence = [obs_map_r[i] for i in emission]\n",
    "    \n",
    "    if start_word:\n",
    "        sentence[0]=obs_map_r[start_word]\n",
    "\n",
    "    return ' '.join(sentence[::-1]).capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "moral-peeing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Sonnet:\n",
      "====================\n",
      "Be that can come is of with burn his aid\n",
      "Keeps i look wrought for never despair gay\n",
      "On live by from is am voices decayed\n",
      "Check mad that tongues i through not world or spend\n",
      "All chide thou use you we bad must subdued\n",
      "Preserve my i by be are friends true shine\n",
      "Make to then than my sweet i thy renewed\n",
      "My love like i for make mine time that mine\n",
      "So dignity the because little come\n",
      "From same winds but put have how my not hour\n",
      "I so twice love all love with growing sum\n",
      "Things made world part with when verse nor his sour\n",
      "Art said she though this my subjects and be\n",
      "With dear such their gilded returned ten thee\n"
     ]
    }
   ],
   "source": [
    "print('Sample Sonnet:\\n====================')\n",
    "sentence_rhyme=['' for i in range(14)]\n",
    "mapper=obs_map_reverser(word_dict)\n",
    "for i in range(3):\n",
    "    new_rhyme_1=random.choice(rhyme)\n",
    "    new_rhyme_2=random.choice(rhyme)\n",
    "    sentence_rhyme[i*4]=new_rhyme_1[0]\n",
    "    sentence_rhyme[i*4+1]=new_rhyme_2[0]\n",
    "    sentence_rhyme[i*4+2]=new_rhyme_1[1]\n",
    "    sentence_rhyme[i*4+3]=new_rhyme_2[1]\n",
    "#     print(new_rhyme_1,new_rhyme_2)\n",
    "new_rhyme_1=random.choice(rhyme_couplet)\n",
    "sentence_rhyme[-2]=new_rhyme_1[0]\n",
    "sentence_rhyme[-1]=new_rhyme_1[1]\n",
    "# print(mapper[new_rhyme_1[0]],mapper[new_rhyme_1[1]])\n",
    "for start_rhyme in sentence_rhyme:\n",
    "    print(sample_rev_sonents_meter(hmm_reverse, word_dict, syllable_dict, prondict, n_sonnet=10, start_word=start_rhyme))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "behavioral-deviation",
   "metadata": {},
   "source": [
    "## Mixing Spenser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "pending-elder",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words, all_sequences, word_dict, all_sonnet_int, syllable_dict= helper.getAllWordsAndSequences(\"data/shakespeare.txt\", \"data/Syllable_dictionary.txt\",syllable_count=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "improving-mention",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words, all_sequences, word_dict, all_sonnet_int, syllable_dict= helper.addSpenserData(\"data/spenser.txt\",all_words, all_sequences, word_dict, all_sonnet_int, syllable_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "deluxe-kenya",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sonet_sequence=[]\n",
    "for idx in range(len(all_sonnet_int)):\n",
    "    if len(all_sonnet_int[idx])>1:\n",
    "        new_sonet_sequence.append(all_sonnet_int[idx][-2::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "global-ocean",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 10\n",
      "Iteration: 20\n",
      "Iteration: 30\n",
      "Iteration: 40\n",
      "Iteration: 50\n",
      "Iteration: 60\n",
      "Iteration: 70\n",
      "Iteration: 80\n",
      "Iteration: 90\n",
      "Iteration: 100\n"
     ]
    }
   ],
   "source": [
    "hmm_spenser = helper.unsupervised_HMM(new_sonet_sequence, 10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "alien-electricity",
   "metadata": {},
   "outputs": [],
   "source": [
    "rhyme=[]\n",
    "rhyme_couplet=[]\n",
    "new_sonet_sequence=[]\n",
    "for idx in range(len(all_sonnet_int)):\n",
    "    if (idx+2)%14==0 and len(all_sonnet_int[idx+1])>1:\n",
    "        rhyme_couplet.append((all_sonnet_int[idx][-2],all_sonnet_int[idx+1][-2]))\n",
    "    elif idx%4==0 and len(all_sonnet_int[idx+2])>1 and len(all_sonnet_int[idx+1])>1:\n",
    "        rhyme.append((all_sonnet_int[idx][-2],all_sonnet_int[idx+2][-2]))\n",
    "        rhyme.append((all_sonnet_int[idx+1][-2],all_sonnet_int[idx+3][-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "italic-agenda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 1)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prondict = cmudict.dict()\n",
    "stress_syllable('compare',prondict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "seasonal-decimal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Sonnet:\n",
      "====================\n",
      "Be old will is love have dost desire\n",
      "Scope i have proud comments dismay their brood\n",
      "Buy self they with my it life she not heat\n",
      "Breast when i my if as lends for mine blood\n",
      "Most i that so that till those chastity\n",
      "To which once me hell not brain of well live\n",
      "You fairly her within one's monument\n",
      "Flame care of as chaste look i deep but give\n",
      "Foe that thus thy do not miss knowing slow\n",
      "Your my which vain let make to bends through leave\n",
      "Forth strong thee castles untainted expense\n",
      "His my woman and another deceive\n",
      "Their as seem prove ill for my fit to prove\n",
      "Be age of love which though can your those love\n"
     ]
    }
   ],
   "source": [
    "print('Sample Sonnet:\\n====================')\n",
    "sentence_rhyme=['' for i in range(14)]\n",
    "mapper=obs_map_reverser(word_dict)\n",
    "for i in range(3):\n",
    "    new_rhyme_1=random.choice(rhyme)\n",
    "    new_rhyme_2=random.choice(rhyme)\n",
    "    sentence_rhyme[i*4]=new_rhyme_1[0]\n",
    "    sentence_rhyme[i*4+1]=new_rhyme_2[0]\n",
    "    sentence_rhyme[i*4+2]=new_rhyme_1[1]\n",
    "    sentence_rhyme[i*4+3]=new_rhyme_2[1]\n",
    "#     print(new_rhyme_1,new_rhyme_2)\n",
    "new_rhyme_1=random.choice(rhyme_couplet)\n",
    "sentence_rhyme[-2]=new_rhyme_1[0]\n",
    "sentence_rhyme[-1]=new_rhyme_1[1]\n",
    "# print(mapper[new_rhyme_1[0]],mapper[new_rhyme_1[1]])\n",
    "for start_rhyme in sentence_rhyme:\n",
    "#     print(mapper[start_rhyme])\n",
    "    print(sample_rev_sonents_meter(hmm_spenser, word_dict, syllable_dict, prondict, n_sonnet=10, start_word=start_rhyme))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
